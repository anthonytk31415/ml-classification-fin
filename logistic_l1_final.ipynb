{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa96c50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset preview:\n",
      "   age           job  marital  education default  balance housing loan  \\\n",
      "0   58    management  married   tertiary      no     2143     yes   no   \n",
      "1   44    technician   single  secondary      no       29     yes   no   \n",
      "2   33  entrepreneur  married  secondary      no        2     yes  yes   \n",
      "3   47   blue-collar  married    unknown      no     1506     yes   no   \n",
      "4   33       unknown   single    unknown      no        1      no   no   \n",
      "\n",
      "   contact  day month  duration  campaign  pdays  previous poutcome   y  \n",
      "0  unknown    5   may       261         1     -1         0  unknown  no  \n",
      "1  unknown    5   may       151         1     -1         0  unknown  no  \n",
      "2  unknown    5   may        76         1     -1         0  unknown  no  \n",
      "3  unknown    5   may        92         1     -1         0  unknown  no  \n",
      "4  unknown    5   may       198         1     -1         0  unknown  no  \n",
      "\n",
      "Dataset shape: (45211, 17)\n",
      "\n",
      "Categorical columns: ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n",
      "\n",
      "Numerical columns: ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']\n",
      "\n",
      "Feature names after one-hot encoding:\n",
      "0: age\n",
      "1: balance\n",
      "2: day\n",
      "3: duration\n",
      "4: campaign\n",
      "5: pdays\n",
      "6: previous\n",
      "7: job_blue-collar\n",
      "8: job_entrepreneur\n",
      "9: job_housemaid\n",
      "10: job_management\n",
      "11: job_retired\n",
      "12: job_self-employed\n",
      "13: job_services\n",
      "14: job_student\n",
      "15: job_technician\n",
      "16: job_unemployed\n",
      "17: job_unknown\n",
      "18: marital_married\n",
      "19: marital_single\n",
      "20: education_secondary\n",
      "21: education_tertiary\n",
      "22: education_unknown\n",
      "23: default_yes\n",
      "24: housing_yes\n",
      "25: loan_yes\n",
      "26: contact_telephone\n",
      "27: contact_unknown\n",
      "28: month_aug\n",
      "29: month_dec\n",
      "30: month_feb\n",
      "31: month_jan\n",
      "32: month_jul\n",
      "33: month_jun\n",
      "34: month_mar\n",
      "35: month_may\n",
      "36: month_nov\n",
      "37: month_oct\n",
      "38: month_sep\n",
      "39: poutcome_other\n",
      "40: poutcome_success\n",
      "41: poutcome_unknown\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "input_df = pd.read_csv(\"dataset/bank-full.csv\", sep=';')\n",
    "\n",
    "# Print the first few rows to understand the data\n",
    "print(\"Dataset preview:\")\n",
    "print(input_df.head())\n",
    "print(\"\\nDataset shape:\", input_df.shape)\n",
    "\n",
    "# Separate features and target\n",
    "target_column = 'y'\n",
    "y = (input_df[target_column] == 'yes').astype(int)\n",
    "X = input_df.drop(target_column, axis=1)\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "categorical_columns = [\"job\", \"marital\", \"education\", \"default\", \"housing\", \"loan\", \"contact\", \"month\", \"poutcome\"]\n",
    "numerical_columns = [col for col in X.columns if col not in categorical_columns]\n",
    "\n",
    "print(\"\\nCategorical columns:\", categorical_columns)\n",
    "print(\"\\nNumerical columns:\", numerical_columns)\n",
    "\n",
    "# Create a preprocessing pipeline with one-hot encoding and normalization\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_columns),\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_columns)\n",
    "    ])\n",
    "\n",
    "# Fit and transform the data\n",
    "X_transformed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Get the feature names after one-hot encoding\n",
    "ohe_feature_names = []\n",
    "for i, encoder in enumerate(preprocessor.transformers_):\n",
    "    if encoder[0] == 'cat':\n",
    "        encoder_obj = encoder[1]\n",
    "        feature_names = encoder_obj.get_feature_names_out(categorical_columns)\n",
    "        ohe_feature_names.extend(feature_names)\n",
    "    else:\n",
    "        ohe_feature_names.extend(numerical_columns)\n",
    "\n",
    "print(\"\\nFeature names after one-hot encoding:\")\n",
    "for i, name in enumerate(ohe_feature_names):\n",
    "    print(f\"{i}: {name}\")\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_transformed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train.toarray() if hasattr(X_train, 'toarray') else X_train)\n",
    "X_test_tensor = torch.FloatTensor(X_test.toarray() if hasattr(X_test, 'toarray') else X_test)\n",
    "y_train_tensor = torch.FloatTensor(y_train.values).reshape(-1, 1)\n",
    "y_test_tensor = torch.FloatTensor(y_test.values).reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1ff53ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 0.3292\n",
      "Epoch [200/1000], Loss: 0.3169\n",
      "Epoch [300/1000], Loss: 0.3141\n",
      "Epoch [400/1000], Loss: 0.3132\n",
      "Epoch [500/1000], Loss: 0.3130\n",
      "Epoch [600/1000], Loss: 0.3128\n",
      "Epoch [700/1000], Loss: 0.3130\n",
      "Epoch [800/1000], Loss: 0.3129\n",
      "Epoch [900/1000], Loss: 0.3129\n",
      "Epoch [1000/1000], Loss: 0.3129\n",
      "\n",
      "Test Accuracy: 0.8861\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.98      0.94      7952\n",
      "         1.0       0.60      0.17      0.27      1091\n",
      "\n",
      "    accuracy                           0.89      9043\n",
      "   macro avg       0.75      0.58      0.60      9043\n",
      "weighted avg       0.86      0.89      0.86      9043\n",
      "\n",
      "\n",
      "Model coefficients (feature weights):\n",
      "Bias term: -0.8664\n",
      "poutcome_unknown: -1.0030\n",
      "duration: 0.8393\n",
      "housing_yes: -0.7632\n",
      "contact_unknown: -0.3531\n",
      "poutcome_success: 0.2853\n",
      "marital_married: -0.1318\n",
      "campaign: -0.0442\n",
      "month_may: -0.0099\n",
      "job_unknown: -0.0040\n",
      "month_aug: -0.0035\n",
      "month_oct: 0.0035\n",
      "job_services: 0.0032\n",
      "month_mar: 0.0028\n",
      "job_housemaid: -0.0027\n",
      "pdays: -0.0027\n",
      "job_blue-collar: -0.0024\n",
      "education_unknown: 0.0024\n",
      "job_entrepreneur: -0.0022\n",
      "month_dec: 0.0021\n",
      "loan_yes: -0.0018\n",
      "previous: 0.0018\n",
      "month_jun: -0.0017\n",
      "job_self-employed: -0.0017\n",
      "month_jan: -0.0016\n",
      "job_management: -0.0014\n",
      "balance: 0.0012\n",
      "age: 0.0012\n",
      "job_retired: -0.0011\n",
      "day: 0.0010\n",
      "education_secondary: -0.0009\n",
      "contact_telephone: -0.0009\n",
      "job_student: -0.0008\n",
      "education_tertiary: 0.0007\n",
      "poutcome_other: -0.0005\n",
      "default_yes: 0.0005\n",
      "month_sep: 0.0004\n",
      "month_feb: 0.0004\n",
      "job_unemployed: -0.0003\n",
      "month_nov: -0.0002\n",
      "marital_single: 0.0002\n",
      "job_technician: -0.0001\n",
      "month_jul: 0.0001\n",
      "\n",
      "Number of non-zero weights: 42 out of 42\n",
      "Percentage of weights zeroed out: 0.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the logistic regression model with L1 regularization\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(self.linear(x))\n",
    "\n",
    "# Get input dimension from transformed data\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "model = LogisticRegressionModel(input_dim)\n",
    "\n",
    "# Define loss function and optimizer with L1 regularization\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 1000\n",
    "l1_lambda = 0.01  # L1 regularization strength\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    \n",
    "    # Add L1 regularization\n",
    "    l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "    loss = loss + l1_lambda * l1_norm\n",
    "    \n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluate the model\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    y_pred_prob = model(X_test_tensor)\n",
    "    y_pred = (y_pred_prob > 0.5).float()\n",
    "    accuracy = accuracy_score(y_test_tensor, y_pred)\n",
    "    print(f'\\nTest Accuracy: {accuracy:.4f}')\n",
    "    print('\\nClassification Report:')\n",
    "    print(classification_report(y_test_tensor, y_pred))\n",
    "\n",
    "# Extract and print feature weights\n",
    "weights = model.linear.weight.data.numpy().flatten()\n",
    "bias = model.linear.bias.data.numpy()[0]\n",
    "\n",
    "print(\"\\nModel coefficients (feature weights):\")\n",
    "print(f\"Bias term: {bias:.4f}\")\n",
    "\n",
    "# Sort weights by absolute value for interpretability\n",
    "sorted_indices = np.argsort(np.abs(weights))[::-1]\n",
    "for i, idx in enumerate(sorted_indices):\n",
    "    if i < len(ohe_feature_names):\n",
    "        print(f\"{ohe_feature_names[idx]}: {weights[idx]:.4f}\")\n",
    "    else:\n",
    "        print(f\"Feature {idx}: {weights[idx]:.4f}\")\n",
    "\n",
    "# Examine which features have non-zero weights (L1 regularization should induce sparsity)\n",
    "non_zero_weights = weights[np.abs(weights) > 1e-4]\n",
    "print(f\"\\nNumber of non-zero weights: {len(non_zero_weights)} out of {len(weights)}\")\n",
    "print(f\"Percentage of weights zeroed out: {(1 - len(non_zero_weights)/len(weights)) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa769fa2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
