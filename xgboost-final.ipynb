{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b04bc9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset preview:\n",
      "   age           job  marital  education default  balance housing loan  \\\n",
      "0   58    management  married   tertiary      no     2143     yes   no   \n",
      "1   44    technician   single  secondary      no       29     yes   no   \n",
      "2   33  entrepreneur  married  secondary      no        2     yes  yes   \n",
      "3   47   blue-collar  married    unknown      no     1506     yes   no   \n",
      "4   33       unknown   single    unknown      no        1      no   no   \n",
      "\n",
      "   contact  day month  duration  campaign  pdays  previous poutcome   y  \n",
      "0  unknown    5   may       261         1     -1         0  unknown  no  \n",
      "1  unknown    5   may       151         1     -1         0  unknown  no  \n",
      "2  unknown    5   may        76         1     -1         0  unknown  no  \n",
      "3  unknown    5   may        92         1     -1         0  unknown  no  \n",
      "4  unknown    5   may       198         1     -1         0  unknown  no  \n",
      "\n",
      "Dataset shape: (45211, 17)\n",
      "\n",
      "Categorical columns: ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n",
      "\n",
      "Numerical columns: ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']\n",
      "\n",
      "Feature names after one-hot encoding:\n",
      "0: age\n",
      "1: balance\n",
      "2: day\n",
      "3: duration\n",
      "4: campaign\n",
      "5: pdays\n",
      "6: previous\n",
      "7: job_blue-collar\n",
      "8: job_entrepreneur\n",
      "9: job_housemaid\n",
      "10: job_management\n",
      "11: job_retired\n",
      "12: job_self-employed\n",
      "13: job_services\n",
      "14: job_student\n",
      "15: job_technician\n",
      "16: job_unemployed\n",
      "17: job_unknown\n",
      "18: marital_married\n",
      "19: marital_single\n",
      "20: education_secondary\n",
      "21: education_tertiary\n",
      "22: education_unknown\n",
      "23: default_yes\n",
      "24: housing_yes\n",
      "25: loan_yes\n",
      "26: contact_telephone\n",
      "27: contact_unknown\n",
      "28: month_aug\n",
      "29: month_dec\n",
      "30: month_feb\n",
      "31: month_jan\n",
      "32: month_jul\n",
      "33: month_jun\n",
      "34: month_mar\n",
      "35: month_may\n",
      "36: month_nov\n",
      "37: month_oct\n",
      "38: month_sep\n",
      "39: poutcome_other\n",
      "40: poutcome_success\n",
      "41: poutcome_unknown\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from get_dataset import X_train_tensor as X_train, X_test_tensor as X_test, y_train_tensor as y_train, y_test_tensor as y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8348898d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cd25d0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training standard XGBoost model...\n",
      "XGBoost Model Accuracy: 0.9080\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.97      0.95      7952\n",
      "         1.0       0.66      0.49      0.56      1091\n",
      "\n",
      "    accuracy                           0.91      9043\n",
      "   macro avg       0.80      0.73      0.76      9043\n",
      "weighted avg       0.90      0.91      0.90      9043\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[7675  277]\n",
      " [ 555  536]]\n",
      "\n",
      "Training XGBoost with PyTorch DataLoader...\n",
      "\n",
      "Feature Importance from XGBoost:\n",
      "Feature 0: 0.0107\n",
      "Feature 1: 0.0070\n",
      "Feature 2: 0.0111\n",
      "Feature 3: 0.0466\n",
      "Feature 4: 0.0090\n",
      "Feature 5: 0.0108\n",
      "Feature 6: 0.0080\n",
      "Feature 7: 0.0132\n",
      "Feature 8: 0.0054\n",
      "Feature 9: 0.0069\n",
      "Feature 10: 0.0060\n",
      "Feature 11: 0.0049\n",
      "Feature 12: 0.0053\n",
      "Feature 13: 0.0048\n",
      "Feature 14: 0.0064\n",
      "Feature 15: 0.0056\n",
      "Feature 16: 0.0045\n",
      "Feature 17: 0.0045\n",
      "Feature 18: 0.0094\n",
      "Feature 19: 0.0104\n",
      "Feature 20: 0.0058\n",
      "Feature 21: 0.0075\n",
      "Feature 22: 0.0061\n",
      "Feature 23: 0.0050\n",
      "Feature 24: 0.0339\n",
      "Feature 25: 0.0180\n",
      "Feature 26: 0.0052\n",
      "Feature 27: 0.0705\n",
      "Feature 28: 0.0108\n",
      "Feature 29: 0.0142\n",
      "Feature 30: 0.0154\n",
      "Feature 31: 0.0134\n",
      "Feature 32: 0.0194\n",
      "Feature 33: 0.0242\n",
      "Feature 34: 0.0431\n",
      "Feature 35: 0.0136\n",
      "Feature 36: 0.0136\n",
      "Feature 37: 0.0298\n",
      "Feature 38: 0.0236\n",
      "Feature 39: 0.0055\n",
      "Feature 40: 0.1839\n",
      "Feature 41: 0.2470\n",
      "\n",
      "XGBoost Final Test Accuracy: 0.9080\n",
      "\n",
      "Performing hyperparameter tuning (this may take some time)...\n",
      "Fitting 3 folds for each of 243 candidates, totalling 729 fits\n",
      "Best parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.2, 'max_depth': 4, 'n_estimators': 200, 'subsample': 1.0}\n",
      "Best accuracy: 0.9081\n",
      "\n",
      "Performance analysis for max_depth:\n",
      "                 Test Mean  Test Std  Count  Train Mean  Train Std\n",
      "param_max_depth                                                   \n",
      "3                   0.9001    0.0089     81      0.9061     0.0135\n",
      "4                   0.9012    0.0081     81      0.9134     0.0170\n",
      "5                   0.9015    0.0078     81      0.9215     0.0221\n",
      "\n",
      "Performance analysis for learning_rate:\n",
      "                     Test Mean  Test Std  Count  Train Mean  Train Std\n",
      "param_learning_rate                                                   \n",
      "0.01                    0.8904    0.0060     81      0.8925     0.0080\n",
      "0.10                    0.9059    0.0016     81      0.9194     0.0098\n",
      "0.20                    0.9065    0.0007     81      0.9292     0.0136\n",
      "\n",
      "Performance analysis for n_estimators:\n",
      "                    Test Mean  Test Std  Count  Train Mean  Train Std\n",
      "param_n_estimators                                                   \n",
      "50                     0.8980    0.0101     81      0.9047     0.0157\n",
      "100                    0.9010    0.0082     81      0.9128     0.0168\n",
      "200                    0.9038    0.0045     81      0.9235     0.0192\n",
      "\n",
      "Performance analysis for subsample:\n",
      "                 Test Mean  Test Std  Count  Train Mean  Train Std\n",
      "param_subsample                                                   \n",
      "0.8                 0.9010    0.0082     81      0.9140     0.0191\n",
      "0.9                 0.9009    0.0083     81      0.9138     0.0192\n",
      "1.0                 0.9009    0.0084     81      0.9131     0.0186\n",
      "\n",
      "Performance analysis for colsample_bytree:\n",
      "                        Test Mean  Test Std  Count  Train Mean  Train Std\n",
      "param_colsample_bytree                                                   \n",
      "0.8                        0.9008    0.0085     81      0.9132     0.0189\n",
      "0.9                        0.9009    0.0082     81      0.9136     0.0189\n",
      "1.0                        0.9010    0.0082     81      0.9142     0.0191\n",
      "\n",
      "Interaction analysis between max_depth and learning_rate:\n",
      "param_learning_rate    0.01    0.10    0.20\n",
      "param_max_depth                            \n",
      "3                    0.8885  0.9053  0.9065\n",
      "4                    0.8908  0.9061  0.9066\n",
      "5                    0.8920  0.9063  0.9063\n",
      "\n",
      "Interaction analysis between n_estimators and learning_rate:\n",
      "param_learning_rate    0.01    0.10    0.20\n",
      "param_n_estimators                         \n",
      "50                   0.8839  0.9040  0.9062\n",
      "100                  0.8898  0.9065  0.9068\n",
      "200                  0.8976  0.9072  0.9064\n",
      "\n",
      "Interaction analysis between max_depth and n_estimators:\n",
      "param_n_estimators     50      100     200\n",
      "param_max_depth                           \n",
      "3                   0.8974  0.8996  0.9033\n",
      "4                   0.8982  0.9016  0.9037\n",
      "5                   0.8984  0.9019  0.9043\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Train XGBoost model directly\n",
    "print(\"Training standard XGBoost model...\")\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.2,\n",
    "    max_depth=4,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.9,\n",
    "    objective='binary:logistic',\n",
    "    random_state=42\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 3: Make predictions and evaluate the model\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"XGBoost Model Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Step 4: For PyTorch integration, we can use XGBoost's output as features for a PyTorch model\n",
    "# or use PyTorch's DataLoader for handling the data going into XGBoost\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train)\n",
    "y_train_tensor = torch.FloatTensor(y_train)\n",
    "X_test_tensor = torch.FloatTensor(X_test)\n",
    "y_test_tensor = torch.FloatTensor(y_test)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Option 1: Use XGBoost with PyTorch DataLoader\n",
    "print(\"\\nTraining XGBoost with PyTorch DataLoader...\")\n",
    "def train_xgb_with_pytorch_loader(train_loader):\n",
    "    # Collect all batches\n",
    "    all_X = []\n",
    "    all_y = []\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        all_X.append(X_batch.numpy())\n",
    "        all_y.append(y_batch.numpy())\n",
    "    \n",
    "    # Concatenate batches\n",
    "    X_train_combined = np.vstack(all_X)\n",
    "    y_train_combined = np.concatenate(all_y)\n",
    "    \n",
    "    # Train XGBoost\n",
    "    xgb_model_from_loader = xgb.XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=4,\n",
    "        objective='binary:logistic',\n",
    "        random_state=42\n",
    "    )\n",
    "    xgb_model_from_loader.fit(X_train_combined, y_train_combined)\n",
    "    return xgb_model_from_loader\n",
    "\n",
    "xgb_model_pytorch = train_xgb_with_pytorch_loader(train_loader)\n",
    "\n",
    "# Option 2: Define a simple PyTorch model that uses XGBoost features\n",
    "class LoanDefaultClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(LoanDefaultClassifier, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, 64)\n",
    "        self.fc2 = torch.nn.Linear(64, 32)\n",
    "        self.fc3 = torch.nn.Linear(32, 1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# Extract feature importance from XGBoost\n",
    "print(\"\\nFeature Importance from XGBoost:\")\n",
    "feature_importance = xgb_model.feature_importances_\n",
    "for i, importance in enumerate(feature_importance):\n",
    "    print(f\"Feature {i}: {importance:.4f}\")\n",
    "\n",
    "# Function to evaluate both models on test data\n",
    "def evaluate_models(xgb_model, test_loader):\n",
    "    # Evaluate XGBoost\n",
    "    X_test_np = X_test\n",
    "    y_test_np = y_test\n",
    "    y_pred_xgb = xgb_model.predict(X_test_np)\n",
    "    xgb_accuracy = accuracy_score(y_test_np, y_pred_xgb)\n",
    "    print(f\"\\nXGBoost Final Test Accuracy: {xgb_accuracy:.4f}\")\n",
    "    return xgb_accuracy\n",
    "evaluate_models(xgb_model, test_loader)\n",
    "\n",
    "# Additional code for hyperparameter tuning (optional)\n",
    "# def hyperparameter_tuning():\n",
    "#     from sklearn.model_selection import GridSearchCV\n",
    "    \n",
    "#     param_grid = {\n",
    "#         'max_depth': [3, 4, 5],\n",
    "#         'learning_rate': [0.01, 0.1, 0.2],\n",
    "#         'n_estimators': [50, 100, 200],\n",
    "#         'subsample': [0.8, 0.9, 1.0],\n",
    "#         'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "#     }\n",
    "    \n",
    "#     print(\"\\nPerforming hyperparameter tuning (this may take some time)...\")\n",
    "#     grid_search = GridSearchCV(\n",
    "#         estimator=xgb.XGBClassifier(objective='binary:logistic', random_state=42),\n",
    "#         param_grid=param_grid,\n",
    "#         scoring='accuracy',\n",
    "#         cv=3,\n",
    "#         verbose=1\n",
    "#     )\n",
    "    \n",
    "#     grid_search.fit(X_train, y_train)\n",
    "    \n",
    "#     print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "#     print(f\"Best accuracy: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "#     return grid_search.best_estimator_\n",
    "\n",
    "# # Uncomment to run hyperparameter tuning\n",
    "# best_xgb_model = hyperparameter_tuning()\n",
    "\n",
    "def hyperparameter_tuning():\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    param_grid = {\n",
    "        'max_depth': [3, 4, 5],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'subsample': [0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "    }\n",
    "    \n",
    "    print(\"\\nPerforming hyperparameter tuning (this may take some time)...\")\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=xgb.XGBClassifier(objective='binary:logistic', random_state=42),\n",
    "        param_grid=param_grid,\n",
    "        scoring='accuracy',\n",
    "        cv=3,\n",
    "        verbose=1,\n",
    "        return_train_score=True  # This will give us training scores too\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best accuracy: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    results = pd.DataFrame(grid_search.cv_results_)\n",
    "    \n",
    "    # Function to analyze parameter impact\n",
    "    def analyze_parameter_impact(param_name, results_df):\n",
    "        param_scores = results_df.groupby(f'param_{param_name}').agg({\n",
    "            'mean_test_score': ['mean', 'std', 'count'],\n",
    "            'mean_train_score': ['mean', 'std']\n",
    "        }).round(4)\n",
    "        \n",
    "        param_scores.columns = ['Test Mean', 'Test Std', 'Count', 'Train Mean', 'Train Std']\n",
    "        print(f\"\\nPerformance analysis for {param_name}:\")\n",
    "        print(param_scores)\n",
    "        \n",
    "        # Create visualization\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.errorbar(param_scores.index, param_scores['Test Mean'], \n",
    "                    yerr=param_scores['Test Std'], label='Test Score', \n",
    "                    marker='o', capsize=5)\n",
    "        plt.errorbar(param_scores.index, param_scores['Train Mean'], \n",
    "                    yerr=param_scores['Train Std'], label='Train Score', \n",
    "                    marker='s', capsize=5)\n",
    "        plt.title(f'Performance vs {param_name}')\n",
    "        plt.xlabel(param_name)\n",
    "        plt.ylabel('Score')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f'hyperparameter_analysis_{param_name}.png')\n",
    "        plt.close()\n",
    "        \n",
    "        return param_scores\n",
    "    \n",
    "    # Analyze each parameter\n",
    "    parameter_analyses = {}\n",
    "    for param in param_grid.keys():\n",
    "        parameter_analyses[param] = analyze_parameter_impact(param, results)\n",
    "    \n",
    "    # Create interaction analysis for selected parameter pairs\n",
    "    def analyze_parameter_interaction(param1, param2, results_df):\n",
    "        pivot_table = pd.pivot_table(\n",
    "            results_df,\n",
    "            values='mean_test_score',\n",
    "            index=f'param_{param1}',\n",
    "            columns=f'param_{param2}',\n",
    "            aggfunc='mean'\n",
    "        ).round(4)\n",
    "        \n",
    "        print(f\"\\nInteraction analysis between {param1} and {param2}:\")\n",
    "        print(pivot_table)\n",
    "        \n",
    "        # Create heatmap\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.imshow(pivot_table, cmap='YlOrRd', aspect='auto')\n",
    "        plt.colorbar(label='Score')\n",
    "        plt.title(f'Interaction between {param1} and {param2}')\n",
    "        plt.xlabel(param2)\n",
    "        plt.ylabel(param1)\n",
    "        plt.xticks(range(len(pivot_table.columns)), pivot_table.columns)\n",
    "        plt.yticks(range(len(pivot_table.index)), pivot_table.index)\n",
    "        \n",
    "        # Add text annotations to the heatmap\n",
    "        for i in range(len(pivot_table.index)):\n",
    "            for j in range(len(pivot_table.columns)):\n",
    "                plt.text(j, i, f'{pivot_table.iloc[i, j]:.3f}',\n",
    "                        ha='center', va='center')\n",
    "        \n",
    "        plt.savefig(f'interaction_analysis_{param1}_{param2}.png')\n",
    "        plt.close()\n",
    "        \n",
    "        return pivot_table\n",
    "    \n",
    "    # Analyze interactions between important parameter pairs\n",
    "    interaction_analyses = {}\n",
    "    important_pairs = [\n",
    "        ('max_depth', 'learning_rate'),\n",
    "        ('n_estimators', 'learning_rate'),\n",
    "        ('max_depth', 'n_estimators')\n",
    "    ]\n",
    "    \n",
    "    for param1, param2 in important_pairs:\n",
    "        interaction_analyses[(param1, param2)] = analyze_parameter_interaction(param1, param2, results)\n",
    "    \n",
    "    # Save all results to a comprehensive report\n",
    "    with open('hyperparameter_tuning_report.txt', 'w') as f:\n",
    "        f.write(\"XGBoost Hyperparameter Tuning Report\\n\")\n",
    "        f.write(\"===================================\\n\\n\")\n",
    "        \n",
    "        f.write(\"Best Parameters:\\n\")\n",
    "        f.write(f\"{grid_search.best_params_}\\n\\n\")\n",
    "        \n",
    "        f.write(\"Best Score:\\n\")\n",
    "        f.write(f\"{grid_search.best_score_:.4f}\\n\\n\")\n",
    "        \n",
    "        f.write(\"Individual Parameter Analysis:\\n\")\n",
    "        f.write(\"-----------------------------\\n\")\n",
    "        for param, analysis in parameter_analyses.items():\n",
    "            f.write(f\"\\n{param}:\\n\")\n",
    "            f.write(analysis.to_string())\n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        f.write(\"\\nParameter Interaction Analysis:\\n\")\n",
    "        f.write(\"------------------------------\\n\")\n",
    "        for (param1, param2), analysis in interaction_analyses.items():\n",
    "            f.write(f\"\\n{param1} vs {param2}:\\n\")\n",
    "            f.write(analysis.to_string())\n",
    "            f.write(\"\\n\")\n",
    "    \n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "# Uncomment to run hyperparameter tuning\n",
    "best_xgb_model = hyperparameter_tuning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46ff23d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XGBoost Final Test Accuracy: 0.9080\n"
     ]
    }
   ],
   "source": [
    "acc = evaluate_models(xgb_model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318f99f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
