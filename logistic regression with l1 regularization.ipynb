{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8ca079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from get_dataset import X, y\n",
    "\n",
    "# Define the categorical columns\n",
    "X = pd.DataFrame(X)\n",
    "\n",
    "# Define the categorical columns\n",
    "categorical = ['job', 'marital', 'education', 'housing', 'loan', 'contact', 'campaign', 'previous', 'poutcome']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1632530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2143</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>261</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>151</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1506</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>92</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>33</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>198</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1   2   3   4     5   6   7   8   9   10   11  12  13  14  15\n",
       "0  58   4   1   2   0  2143   1   0   2   5   8  261   1  -1   0   3\n",
       "1  44   9   2   1   0    29   1   0   2   5   8  151   1  -1   0   3\n",
       "2  33   2   1   1   0     2   1   1   2   5   8   76   1  -1   0   3\n",
       "3  47   1   1   3   0  1506   1   0   2   5   8   92   1  -1   0   3\n",
       "4  33  11   2   3   0     1   0   0   2   5   8  198   1  -1   0   3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88f103e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['job', 'marital', 'education', 'housing', 'loan', 'contact', 'campaign',\\n       'previous', 'poutcome'],\\n      dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# One-hot encode the categorical features\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m X_encoded \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mget_dummies(X, columns\u001b[38;5;241m=\u001b[39mcategorical)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Initialize the scaler and apply to the entire dataset (after encoding)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pandas/core/reshape/encoding.py:164\u001b[0m, in \u001b[0;36mget_dummies\u001b[0;34m(data, prefix, prefix_sep, dummy_na, columns, sparse, drop_first, dtype)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput must be a list-like for parameter `columns`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     data_to_encode \u001b[38;5;241m=\u001b[39m data[columns]\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# validate prefixes and separator to avoid silently dropping cols\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_len\u001b[39m(item, name: \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pandas/core/frame.py:3899\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3897\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3898\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3899\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3901\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:6114\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6112\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   6116\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6118\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:6175\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6173\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[1;32m   6174\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 6175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6177\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6178\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['job', 'marital', 'education', 'housing', 'loan', 'contact', 'campaign',\\n       'previous', 'poutcome'],\\n      dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "\n",
    "# One-hot encode the categorical features\n",
    "X_encoded = pd.get_dummies(X, columns=categorical)\n",
    "\n",
    "# Initialize the scaler and apply to the entire dataset (after encoding)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the entire dataset after one-hot encoding\n",
    "X_scaled = scaler.fit_transform(X_encoded)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_scaled_train, X_scaled_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the scaled data to torch tensors\n",
    "X_scaled_train = torch.tensor(X_scaled_train, dtype=torch.float32)\n",
    "X_scaled_test = torch.tensor(X_scaled_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)  # Reshape to match the output of sigmoid\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Define the Logistic Regression model with L1 regularization\n",
    "class LogisticRegressionL1(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LogisticRegressionL1, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.linear(x))\n",
    "\n",
    "# L1 regularization function\n",
    "def l1_regularization(model):\n",
    "    l1_norm = 0\n",
    "    for param in model.parameters():\n",
    "        l1_norm += torch.sum(torch.abs(param))\n",
    "    return l1_norm\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "input_dim = X_scaled_train.shape[1]\n",
    "model = LogisticRegressionL1(input_dim)\n",
    "criterion = nn.BCEWithLogitsLoss()  # Binary Cross-Entropy Loss\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 100\n",
    "lambda_l1 = 0.01  # L1 regularization strength\n",
    "\n",
    "# Lists to store loss and accuracy\n",
    "performance = {'train_loss': [], 'test_loss': [], 'train_accuracy': [], 'test_accuracy': []}\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # Forward pass (train set)\n",
    "    outputs_train = model(X_scaled_train)\n",
    "    loss_train = criterion(outputs_train, y_train)\n",
    "    \n",
    "    # Apply L1 regularization\n",
    "    l1_loss = l1_regularization(model)\n",
    "    loss_train += lambda_l1 * l1_loss\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Calculate train accuracy\n",
    "    with torch.no_grad():\n",
    "        predictions_train = (outputs_train > 0.5).float()\n",
    "        train_accuracy = (predictions_train == y_train).float().mean()\n",
    "        \n",
    "        # Calculate test loss and accuracy\n",
    "        outputs_test = model(X_scaled_test)\n",
    "        loss_test = criterion(outputs_test, y_test)\n",
    "        predictions_test = (outputs_test > 0.5).float()\n",
    "        test_accuracy = (predictions_test == y_test).float().mean()\n",
    "    \n",
    "    # Store loss and accuracy\n",
    "    performance['train_loss'].append(loss_train.item())\n",
    "    performance['test_loss'].append(loss_test.item())\n",
    "    performance['train_accuracy'].append(train_accuracy.item())\n",
    "    performance['test_accuracy'].append(test_accuracy.item())\n",
    "\n",
    "# Function to plot performance (same as before)\n",
    "def plot_performance(performance):\n",
    "    '''\n",
    "    Function for plotting training and test losses and accuracies\n",
    "    '''\n",
    "    plt.style.use('seaborn-v0_8-dark')\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(16, 4.5))\n",
    "    \n",
    "    # Plot loss (train vs test)\n",
    "    ax[0].plot(performance['train_loss'], label='Train Loss')\n",
    "    ax[0].plot(performance['test_loss'], label='Test Loss')\n",
    "    ax[0].set(title=\"Loss over epochs\", xlabel='Epoch', ylabel='Loss')\n",
    "    \n",
    "    # Plot accuracy (train vs test)\n",
    "    ax[1].plot(performance['train_accuracy'], label='Train Accuracy')\n",
    "    ax[1].plot(performance['test_accuracy'], label='Test Accuracy')\n",
    "    ax[1].set(title=\"Accuracy over epochs\", xlabel='Epoch', ylabel='Accuracy')\n",
    "\n",
    "    ax[0].legend()\n",
    "    ax[1].legend()\n",
    "\n",
    "    plt.show()\n",
    "    plt.style.use('default')\n",
    "\n",
    "# Plot the performance results\n",
    "plot_performance(performance)\n",
    "\n",
    "\n",
    "# Assuming 'model' is the trained logistic regression model\n",
    "# Print model parameters (weights and bias)\n",
    "# for name, param in model.named_parameters():\n",
    "#     if 'linear' in name:\n",
    "#         print(f\"Parameter: {name} | Shape: {param.shape}\")\n",
    "#         print(param.data)\n",
    "\n",
    "# Get the weights and biases\n",
    "weights = model.linear.weight.data  # Shape: (1, input_dim)\n",
    "bias = model.linear.bias.data  # Shape: (1,)\n",
    "\n",
    "# Print the weights for each feature\n",
    "# print(f\"Weights: {weights}\")\n",
    "# print(f\"Bias: {bias}\")\n",
    "\n",
    "# Convert weights to a numpy array for easier inspection\n",
    "weights = weights.squeeze().cpu().numpy()  # Convert to 1D array if it's a row vector\n",
    "\n",
    "# Visualize the weights (using matplotlib)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 2))\n",
    "plt.bar(range(len(weights)), weights)\n",
    "plt.title(\"Feature Weights after L1 Regularization\")\n",
    "plt.xlabel(\"Feature Index\")\n",
    "plt.ylabel(\"Weight Value\")\n",
    "plt.show()\n",
    "\n",
    "# Check which weights are near zero (this indicates non-relevant features)\n",
    "threshold = .01  # A small threshold to consider weights near zero\n",
    "relevant_features = np.abs(weights) > threshold\n",
    "\n",
    "print(f\"Relevant Features: {np.where(relevant_features)[0]}\")\n",
    "print(f\"Irrelevant Features: {np.where(~relevant_features)[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c50d68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55479f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
